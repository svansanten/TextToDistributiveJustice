{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-06-02T17:03:43.496064Z",
     "end_time": "2024-06-02T17:03:43.519109Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notebook to annotate HLS speeches for relevance\n",
    "### B: string-based labels\n",
    "\n",
    "Codebooks:\n",
    "- B1.0: zero shot\n",
    "- B1.1: one shot\n",
    "- (B1.2: two shot)\n",
    "- B1.0.1: zero shot with specific inclusion of context\n",
    "- B1.1.1: one shot with specific inclusion of context\n",
    "\n",
    "Test for 5 different seeds\n",
    "Batch of 20 sentences\n",
    "Original: 5 iterations; does take very long!\n",
    "Set to 1 iteration; check for class imbalances between labelled elements. OOK: seed zou moeten zorgen dat iedere iteratie gelijk is - dit hoeft niet het geval te zijn, zeker niet als de temperature is aangepast. > kan wel inherent in model zitten\n",
    "\n",
    "Main outcomes: T0 - I1\n",
    "For testing purposes: T0 - (T 0.2) - T 0.6 : (I3)\n",
    "\n",
    "Temperature: 0 - 0.6\n",
    "Temperature only focuses on the output probabilities - should thus be set to 0 to make the output as deterministic as possible.\n",
    "> Do a single test to see what the influence of changing temperature is\n",
    "> Top_p is set to 1; making temperature the primary factor.\n",
    "\n",
    "Hypothesis: accuracy decreases with temperature\n",
    "\n",
    "Model selection:\n",
    " As of 22-05-2024, gpt-4-turbo-2024-04-09 seems to be the only gpt-model that returns a fingerprint in addition to gpt-4o\n",
    "\n",
    "  #model= \"gpt-4-turbo-2024-04-09\"\n",
    "  #model = \"gpt-3.5-turbo-0125\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Import text to annotate\n",
    "Select only relevant columns of the full dataframe, in this case:\n",
    "RELEVANCE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import string based datafile\n",
    "HLS_train = pd.read_csv('data/string/HLS_train_string.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T17:03:46.556396Z",
     "end_time": "2024-06-02T17:03:46.585377Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                    id                                               Text  \\\n0          COP19_japan                         Thank you, Mr. President .   \n1          COP19_japan   On beha lf of the government of Japan , I wou...   \n2          COP19_japan   I would also like to expr ess my d eepest con...   \n3          COP19_japan   Mr. President:  A fair and effective framewor...   \n4          COP19_japan   In this regard, Japan firmly supports the est...   \n...                ...                                                ...   \n1207  COP28_newzealand   New Zealand is proud to suppor t several impo...   \n1208  COP28_newzealand  I am joined by New Zealand’s largest business,...   \n1209  COP28_newzealand  The commitment o f New Zealanders from across ...   \n1210  COP28_newzealand                            Thank you Mr President.   \n1211  COP28_newzealand                                          Kia Kaha    \n\n      Relevance  Principle  Topic  Unit  Shape            RELEVANCE  \\\n0             0          0      0     0      0         Not relevant   \n1             0          0      0     0      0         Not relevant   \n2             0          0      0     0      0         Not relevant   \n3             2          3      1     2      2             Relevant   \n4             1          0      0     0      0  Statement of intent   \n...         ...        ...    ...   ...    ...                  ...   \n1207          0          0      0     0      0         Not relevant   \n1208          0          0      0     0      0         Not relevant   \n1209          0          0      0     0      0         Not relevant   \n1210          0          0      0     0      0         Not relevant   \n1211          0          0      0     0      0         Not relevant   \n\n          PRINCIPLE              TOPIC            UNIT          SHAPE  \n0     not evaluated      not evaluated   not evaluated  not evaluated  \n1     not evaluated      not evaluated   not evaluated  not evaluated  \n2     not evaluated      not evaluated   not evaluated  not evaluated  \n3       utilitarian  new UNFCCC policy  responsibility       equality  \n4     not evaluated      not evaluated   not evaluated  not evaluated  \n...             ...                ...             ...            ...  \n1207  not evaluated      not evaluated   not evaluated  not evaluated  \n1208  not evaluated      not evaluated   not evaluated  not evaluated  \n1209  not evaluated      not evaluated   not evaluated  not evaluated  \n1210  not evaluated      not evaluated   not evaluated  not evaluated  \n1211  not evaluated      not evaluated   not evaluated  not evaluated  \n\n[1212 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Relevance</th>\n      <th>Principle</th>\n      <th>Topic</th>\n      <th>Unit</th>\n      <th>Shape</th>\n      <th>RELEVANCE</th>\n      <th>PRINCIPLE</th>\n      <th>TOPIC</th>\n      <th>UNIT</th>\n      <th>SHAPE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>COP19_japan</td>\n      <td>Thank you, Mr. President .</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>COP19_japan</td>\n      <td>On beha lf of the government of Japan , I wou...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>COP19_japan</td>\n      <td>I would also like to expr ess my d eepest con...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>COP19_japan</td>\n      <td>Mr. President:  A fair and effective framewor...</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>Relevant</td>\n      <td>utilitarian</td>\n      <td>new UNFCCC policy</td>\n      <td>responsibility</td>\n      <td>equality</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>COP19_japan</td>\n      <td>In this regard, Japan firmly supports the est...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Statement of intent</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1207</th>\n      <td>COP28_newzealand</td>\n      <td>New Zealand is proud to suppor t several impo...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>1208</th>\n      <td>COP28_newzealand</td>\n      <td>I am joined by New Zealand’s largest business,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>1209</th>\n      <td>COP28_newzealand</td>\n      <td>The commitment o f New Zealanders from across ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>1210</th>\n      <td>COP28_newzealand</td>\n      <td>Thank you Mr President.</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>1211</th>\n      <td>COP28_newzealand</td>\n      <td>Kia Kaha</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Not relevant</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n      <td>not evaluated</td>\n    </tr>\n  </tbody>\n</table>\n<p>1212 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Select only japan for testing purposes\n",
    "#HLS_train_japan = HLS_train[HLS_train['id']=='COP19_japan']\n",
    "HLS_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T17:03:48.803751Z",
     "end_time": "2024-06-02T17:03:48.831418Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Select only columns containing relevance labels\n",
    "HLS_relevance = HLS_train[['Text', 'RELEVANCE']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T17:03:51.398228Z",
     "end_time": "2024-06-02T17:03:51.406180Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Import necessary files\n",
    "- codebooks\n",
    "- API key\n",
    "- import gpt_annotate_num"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B1.0', 'r', encoding='utf-8') as file:\n",
    "    B10 = file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T17:03:53.381458Z",
     "end_time": "2024-06-02T17:03:53.399664Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# OpenAI key\n",
    "with open('gpt_api_key.txt', 'r') as f:\n",
    "    key = f.read().strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T17:03:54.645391Z",
     "end_time": "2024-06-02T17:03:54.665550Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import gpt_annotate_string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-02T17:03:56.109380Z",
     "end_time": "2024-06-02T17:04:12.195957Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Prepare data for annotation\n",
    "Compares column names in HLS_relevance to the codes identified by GPT-4o in the codebook. Seed for this identification is set to 1234."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Prepare dataframe for annotation\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m text_to_annotate \u001B[38;5;241m=\u001B[39m \u001B[43mgpt_annotate_string\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mHLS_relevance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mB10\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprep_codebook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - Delft University of Technology\\Documenten\\GitHub\\TextToDistributiveJustice\\DJ_GPT_analysis\\gpt_annotate_string.py:116\u001B[0m, in \u001B[0;36mprepare_data\u001B[1;34m(text_to_annotate, codebook, key, prep_codebook, human_labels, no_print_preview)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m###!# Checking for category names is important - ensures checking if codebook is correct\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m##### 6) Make sure category names in codebook exactly match category names in text_to_annotate\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;66;03m# extract category names from codebook\u001B[39;00m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m human_labels:\n\u001B[1;32m--> 116\u001B[0m   col_names_codebook \u001B[38;5;241m=\u001B[39m \u001B[43mget_classification_categories\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcodebook\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m   \u001B[38;5;66;03m# get category names in text_to_annotate\u001B[39;00m\n\u001B[0;32m    118\u001B[0m   df_cols \u001B[38;5;241m=\u001B[39m text_to_annotate\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[1;32m~\\OneDrive - Delft University of Technology\\Documenten\\GitHub\\TextToDistributiveJustice\\DJ_GPT_analysis\\gpt_annotate_string.py:462\u001B[0m, in \u001B[0;36mget_classification_categories\u001B[1;34m(codebook, key)\u001B[0m\n\u001B[0;32m    459\u001B[0m OpenAI\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetenv(key)\n\u001B[0;32m    461\u001B[0m \u001B[38;5;66;03m### Get GPT response and clean response\u001B[39;00m\n\u001B[1;32m--> 462\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mget_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcodebook\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mllm_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;66;03m# print full response\u001B[39;00m\n\u001B[0;32m    465\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "File \u001B[1;32m~\\OneDrive - Delft University of Technology\\Documenten\\GitHub\\TextToDistributiveJustice\\DJ_GPT_analysis\\gpt_annotate_string.py:404\u001B[0m, in \u001B[0;36mget_response\u001B[1;34m(codebook, llm_query, model, temperature, seed, key)\u001B[0m\n\u001B[0;32m    401\u001B[0m max_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4000\u001B[39m\n\u001B[0;32m    403\u001B[0m \u001B[38;5;66;03m# Create function to llm_query GPT - all parameters are the same for each batch\u001B[39;00m\n\u001B[1;32m--> 404\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    405\u001B[0m \u001B[43m  \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# chatgpt: gpt-3.5-turbo # gpt-4: gpt-4\u001B[39;49;00m\n\u001B[0;32m    406\u001B[0m \u001B[43m  \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcodebook\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mllm_query\u001B[49m\u001B[43m}\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[43m  \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    409\u001B[0m \u001B[43m  \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    410\u001B[0m \u001B[43m  \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    411\u001B[0m \u001B[43m  \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[43m  \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[43m  \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\n\u001B[0;32m    414\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;66;03m# Save fingerprint of each analysis\u001B[39;00m\n\u001B[0;32m    417\u001B[0m system_fingerprint \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39msystem_fingerprint\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:590\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    560\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    588\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 590\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    593\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    594\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    595\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    597\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    598\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    599\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    602\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    604\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    605\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    606\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    607\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    608\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    609\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    614\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    616\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    619\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    620\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    621\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    623\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    624\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_base_client.py:1240\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1228\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1235\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1237\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1238\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1239\u001B[0m     )\n\u001B[1;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_base_client.py:1005\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1003\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1004\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1005\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1006\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1007\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_base_client.py:1053\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1050\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1054\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1055\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1056\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1057\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1058\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1059\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_base_client.py:1005\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1003\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1004\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1005\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1006\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1007\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m        \u001B[49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_base_client.py:1053\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1050\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1054\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1055\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1056\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1057\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1058\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1059\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\App\\Python\\Lib\\site-packages\\openai\\_base_client.py:1020\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1017\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1019\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1020\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1023\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1024\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1027\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1028\u001B[0m )\n",
      "\u001B[1;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Prepare dataframe for annotation\n",
    "text_to_annotate = gpt_annotate_string.prepare_data(HLS_relevance, B10, key, prep_codebook=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T22:44:57.753662Z",
     "end_time": "2024-05-28T22:45:01.566980Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fingerprint used: fp_43dfabdef1\n",
    "\n",
    "Seed of textpreparation is hardcoded into gpt_annotate. This to ensure that onlye the results of the same fingerprint for all seeds and all iterations. Essentially every time GPT-4o is called only results with this specific fingerprint are saved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Run gpt_annotate_string\n",
    "Evaluation per seed -\n",
    "5 different seeds\n",
    "Batch of 20 sentences\n",
    "1 iteration\n",
    "\n",
    "Returns 3 outputs:\n",
    "1. all_iterations_{seed}.csv\n",
    "2. fingerprints_all.csv\n",
    "3. missed_batches.csv\n",
    "\n",
    "## B1.0 Relevance - zero shot - T0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "fingerprint = 'fp_43dfabdef1'\n",
    "\n",
    "# Turn seed of to prevent accidental run of GPT annotate\n",
    "#seeds = [3644,3441, 280, 5991, 7917]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T22:45:09.476196Z",
     "end_time": "2024-05-28T22:45:09.491851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3644 - iteration 1\n",
      "3644 - I1 - B22 fingerprint does not match\n",
      "3644 - I1 - B30 fingerprint does not match\n",
      "3644 - I1 - B32 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "3441 - iteration 1\n",
      "3441 - I1 - B28 fingerprint does not match\n",
      "3441 - I1 - B37 fingerprint does not match\n",
      "3441 - I1 - B60 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "280 - iteration 1\n",
      "280 - I1 - B1 fingerprint does not match\n",
      "280 - I1 - B4 fingerprint does not match\n",
      "280 - I1 - B5 fingerprint does not match\n",
      "280 - I1 - B7 fingerprint does not match\n",
      "280 - I1 - B16 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "5991 - iteration 1\n",
      "5991 - I1 - B54 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "7917 - iteration 1\n",
      "iteration:  1 completed\n"
     ]
    }
   ],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B10, key, seed,fingerprint, experiment=\"B1.0\",  num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T22:45:15.413058Z",
     "end_time": "2024-05-28T23:00:55.093380Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.0 Relevance - zero shot - T0.6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3644 - iteration 1\n",
      "3644 - I1 - B5 fingerprint does not match\n",
      "3644 - I1 - B26 fingerprint does not match\n",
      "3644 - I1 - B39 fingerprint does not match\n",
      "3644 - I1 - B55 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "3441 - iteration 1\n",
      "3441 - I1 - B22 fingerprint does not match\n",
      "3441 - I1 - B29 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "280 - iteration 1\n",
      "280 - I1 - B1 fingerprint does not match\n",
      "280 - I1 - B33 fingerprint does not match\n",
      "280 - I1 - B42 fingerprint does not match\n",
      "280 - I1 - B54 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "5991 - iteration 1\n",
      "5991 - I1 - B32 fingerprint does not match\n",
      "5991 - I1 - B43 fingerprint does not match\n",
      "iteration:  1 completed\n",
      "7917 - iteration 1\n",
      "7917 - I1 - B1 fingerprint does not match\n",
      "7917 - I1 - B12 fingerprint does not match\n",
      "7917 - I1 - B32 fingerprint does not match\n",
      "iteration:  1 completed\n"
     ]
    }
   ],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B10, key, seed,fingerprint, experiment=\"B1.0\",  num_iterations=1, model=\"gpt-4o\", temperature=0.6, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.0 Relevance - zero shot - T0 - I3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I3\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B10, key, seed,fingerprint, experiment=\"B1.0\",  num_iterations=3, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.0.1 Relevance - zero shot - with context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B1.0.1', 'r', encoding='utf-8') as file:\n",
    "    B101 = file.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B101, key, seed, fingerprint, experiment=\"B1.0.1\",num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.1 Relevance - one shot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B1.1', 'r', encoding='utf-8') as file:\n",
    "    B11 = file.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B11, key, seed,fingerprint, experiment=\"B1.1\",  num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.1.1 Relevance - one shot - with context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B1.1.1', 'r', encoding='utf-8') as file:\n",
    "    B111 = file.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B111, key, seed,fingerprint, experiment=\"B1.1.1\",  num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.2 - TWO-shot codebook\n",
    "Codebook is created. Currently not evaluated.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Define evaluation functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Define similarity score function and save to DF\n",
    "def get_similarity_score(Rx, Ry):\n",
    "    # Ensure Rx and Ry are pandas Series and convert to strings with stripped whitespace\n",
    "    Rx = Rx.astype(str).str.strip()\n",
    "    Ry = Ry.astype(str).str.strip()\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = Rx.combine(Ry, lambda x, y: difflib.SequenceMatcher(None, x, y).ratio())\n",
    "\n",
    "    # Apply the threshold - maybe put higher?\n",
    "    similarity_scores = similarity_scores.apply(lambda x: x if x >= 0.9 else 0)\n",
    "\n",
    "    # Return the mean similarity score as a percentage - no need for mean as 1 iteration is used\n",
    "    return similarity_scores.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T23:15:30.313165Z",
     "end_time": "2024-05-28T23:15:30.317215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def similarity(directory):\n",
    "    # Iterate through each file in the directory\n",
    "    list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        Rx = df['RELEVANCE_x']\n",
    "        Ry = df['RELEVANCE_y']\n",
    "\n",
    "        similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "        #Save the score in a dataframe\n",
    "        list.append((filename, similarity_score))\n",
    "\n",
    "    similarity = pd.DataFrame(list, columns=['filename', 'similarity ALL'])\n",
    "    return similarity\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T23:15:31.049758Z",
     "end_time": "2024-05-28T23:15:31.078033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def recall(directory, relevance_x):\n",
    "    # Iterate through each file in the directory\n",
    "    list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        relevant_df = df[df['RELEVANCE_x'] == relevance_x]\n",
    "\n",
    "        Rx = relevant_df['RELEVANCE_x']\n",
    "        Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "        similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "        #Save the score in a dataframe\n",
    "        list.append((filename, similarity_score))\n",
    "\n",
    "    recall = pd.DataFrame(list, columns=['filename', f'{relevance_x} recall'])\n",
    "    return recall"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T23:15:33.867784Z",
     "end_time": "2024-05-28T23:15:33.869792Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def precision(directory, relevance_y):\n",
    "    # Iterate through each file in the directory\n",
    "    list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        relevant_df = df[df['RELEVANCE_y'] == relevance_y]\n",
    "\n",
    "        Rx = relevant_df['RELEVANCE_x']\n",
    "        Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "        similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "        #Save the score in a dataframe\n",
    "        list.append((filename, similarity_score))\n",
    "\n",
    "    precision = pd.DataFrame(list, columns=['filename', f'{relevance_y} precision'])\n",
    "    return precision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T23:15:34.272163Z",
     "end_time": "2024-05-28T23:15:34.280981Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Evaluate performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "                              filename  Relevant recall\n0   all_iterations_string_T0.6_280.csv         0.796117\n1  all_iterations_string_T0.6_3441.csv         0.805556\n2  all_iterations_string_T0.6_3644.csv         0.845771\n3  all_iterations_string_T0.6_5991.csv         0.759615\n4  all_iterations_string_T0.6_7917.csv         0.797101\n5     all_iterations_string_T0_280.csv         0.821990\n6    all_iterations_string_T0_3441.csv         0.815166\n7    all_iterations_string_T0_3644.csv         0.798077\n8    all_iterations_string_T0_5991.csv         0.790698\n9    all_iterations_string_T0_7917.csv         0.807339",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>Relevant recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>all_iterations_string_T0.6_280.csv</td>\n      <td>0.796117</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>all_iterations_string_T0.6_3441.csv</td>\n      <td>0.805556</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>all_iterations_string_T0.6_3644.csv</td>\n      <td>0.845771</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>all_iterations_string_T0.6_5991.csv</td>\n      <td>0.759615</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>all_iterations_string_T0.6_7917.csv</td>\n      <td>0.797101</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>all_iterations_string_T0_280.csv</td>\n      <td>0.821990</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>all_iterations_string_T0_3441.csv</td>\n      <td>0.815166</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>all_iterations_string_T0_3644.csv</td>\n      <td>0.798077</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>all_iterations_string_T0_5991.csv</td>\n      <td>0.790698</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>all_iterations_string_T0_7917.csv</td>\n      <td>0.807339</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through each file in the directory\n",
    "B10 = 'STRING_RESULT/B1.0/all_iterations'\n",
    "relevant = 'Relevant'\n",
    "SOI = 'Statement of intent'\n",
    "NR = 'Not relevant'\n",
    "\n",
    "B10_similarity = similarity(B10)\n",
    "B10_recall = recall(B10, relevant)\n",
    "B10_recall\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T23:17:09.054711Z",
     "end_time": "2024-05-28T23:17:11.416125Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
