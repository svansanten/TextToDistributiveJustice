{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:40:45.265650Z",
     "end_time": "2024-05-28T17:40:45.320715Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notebook to annotate HLS speeches for principles\n",
    "### B: string-based labels\n",
    "\n",
    "Codebooks:\n",
    "- B2.0: zero shot\n",
    "- B2.1: one shot\n",
    "\n",
    "Only apply on sentences in OG dataframe labelled as relevant\n",
    "Temperature: 0\n",
    "Iterations: 1\n",
    "\n",
    "Model selection:\n",
    " As of 22-05-2024, gpt-4-turbo-2024-04-09 seems to be the only gpt-model that returns a fingerprint in addition to gpt-4o\n",
    "\n",
    "  #model= \"gpt-4-turbo-2024-04-09\"\n",
    "  #model = \"gpt-3.5-turbo-0125\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import text to annotate\n",
    "Select only relevant columns of the full dataframe, in this case:\n",
    "RELEVANCE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import string based datafile\n",
    "HLS_train = pd.read_csv('data/string/HLS_train_string.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:40:48.427632Z",
     "end_time": "2024-05-28T17:40:48.458963Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "### Select only japan for testing purposes\n",
    "#HLS_train_japan = HLS_train[HLS_train['id']=='COP19_japan']\n",
    "HLS_train_relevant = HLS_train[HLS_train['RELEVANCE']=='Relevant']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:40:51.397531Z",
     "end_time": "2024-05-28T17:40:51.434095Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                Text      PRINCIPLE\n0                         Thank you, Mr. President .  not evaluated\n1   On beha lf of the government of Japan , I wou...  not evaluated\n2   I would also like to expr ess my d eepest con...  not evaluated\n3   Mr. President:  A fair and effective framewor...    utilitarian\n4   In this regard, Japan firmly supports the est...  not evaluated",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>PRINCIPLE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thank you, Mr. President .</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>On beha lf of the government of Japan , I wou...</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I would also like to expr ess my d eepest con...</td>\n      <td>not evaluated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mr. President:  A fair and effective framewor...</td>\n      <td>utilitarian</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>In this regard, Japan firmly supports the est...</td>\n      <td>not evaluated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only columns containing relevance labels\n",
    "HLS_principle = HLS_train[['Text', 'PRINCIPLE']]\n",
    "HLS_principle.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:40:52.390807Z",
     "end_time": "2024-05-28T17:40:52.499569Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import necessary files\n",
    "- codebooks\n",
    "- API key\n",
    "- import gpt_annotate_num"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B2.0', 'r', encoding='utf-8') as file:\n",
    "    B20 = file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:40:54.329064Z",
     "end_time": "2024-05-28T17:40:54.373507Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# OpenAI key\n",
    "with open('gpt_api_key.txt', 'r') as f:\n",
    "    key = f.read().strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:40:55.074084Z",
     "end_time": "2024-05-28T17:40:55.108966Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import gpt_annotate_string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:40:56.193046Z",
     "end_time": "2024-05-28T17:41:17.943659Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare data for annotation\n",
    "Compares column names in HLS_relevance to the codes identified by GPT-4o in the codebook. Seed for this identification is set to 1234."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9TtFOSpeXilg3CpKwme9h5TjSrCCc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='general normative statement, egalitarian, utilitarian, prioritarian, sufficientarian, libertarian', role='assistant', function_call=None, tool_calls=None))], created=1716910910, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_43dfabdef1', usage=CompletionUsage(completion_tokens=18, prompt_tokens=572, total_tokens=590))\n",
      "ERROR: Column names in codebook and text_to_annotate do not match exactly. Please note that order and capitalization matters.\n",
      "Change order/spelling in codebook or text_to_annotate.\n",
      "\n",
      "Exact order and spelling of category names in text_to_annotate:  ['PRINCIPLE']\n",
      "Exact order and spelling of category names in codebook:  ['general normative statement', 'egalitarian', 'utilitarian', 'prioritarian', 'sufficientarian', 'libertarian']\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataframe for annotation\n",
    "text_to_annotate = gpt_annotate_string.prepare_data(HLS_principle, B20, key, prep_codebook=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:41:49.878264Z",
     "end_time": "2024-05-28T17:41:51.153841Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fingerprint used: fp_43dfabdef1\n",
    "\n",
    "Seed of textpreparation is hardcoded into gpt_annotate. This to ensure that onlye the results of the same fingerprint for all seeds and all iterations. Essentially every time GPT-4o is called only results with this specific fingerprint are saved."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run gpt_annotate_num\n",
    "Evaluation per seed -\n",
    "5 different seeds\n",
    "Batch of 20 sentences\n",
    "5 iterations\n",
    "\n",
    "Returns 3 outputs:\n",
    "1. all_iterations_{seed}.csv\n",
    "2. fingerprints_all.csv\n",
    "3. missed_batches.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fingerprint = 'fp_43dfabdef1'\n",
    "seeds = [3644,3441, 280, 5991, 7917]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:14:16.363155Z",
     "end_time": "2024-05-28T15:14:16.384832Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B10, key, seed,fingerprint, experiment=\"B1.0\",  num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T12:29:34.825030Z",
     "end_time": "2024-05-28T12:45:38.983990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate with temperature 0.6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B10, key, seed,fingerprint, experiment=\"B1.0\",  num_iterations=1, model=\"gpt-4o\", temperature=0.6, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate accuracy of predictions\n",
    "Based on similarity between predicted and true labels.\n",
    "Currently not taking consistency into account - would be done by grouping\n",
    "\n",
    "\n",
    "Define function for similarity score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_similarity_score(Rx, Ry):\n",
    "    # Ensure Rx and Ry are pandas Series and convert to strings with stripped whitespace\n",
    "    Rx = Rx.astype(str).str.strip()\n",
    "    Ry = Ry.astype(str).str.strip()\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = Rx.combine(Ry, lambda x, y: difflib.SequenceMatcher(None, x, y).ratio())\n",
    "\n",
    "    # Apply the threshold - maybe put higher?\n",
    "    similarity_scores = similarity_scores.apply(lambda x: x if x >= 0.9 else 0)\n",
    "\n",
    "    # Return the mean similarity score as a percentage\n",
    "    return similarity_scores.mean() * 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:14:22.314056Z",
     "end_time": "2024-05-28T15:14:22.325877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate through each file in the directory\n",
    "directory = 'STRING_RESULT/B1.0/all_iterations'\n",
    "similarity_scores_all = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    Rx = df['RELEVANCE_x']\n",
    "    Ry = df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    similarity_scores_all.append((filename, similarity_score))\n",
    "\n",
    "similarity_all = pd.DataFrame(similarity_scores_all, columns=['filename', 'similarity ALL'])\n",
    "similarity_all.to_csv(\"STRING_RESULT/B1.0/T0_similarity_scores_all\", index=False)\n",
    "\n",
    "similarity_all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:14:34.918874Z",
     "end_time": "2024-05-28T15:14:36.085882Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_scores_relevant = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    similarity_scores_relevant.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "similarity_relevant = pd.DataFrame(similarity_scores_relevant, columns=['filename', 'Similarity RELEVANT'])\n",
    "similarity_relevant.to_csv(\"STRING_RESULT/B1.0/T0_similarity_scores_relevant\", index=False)\n",
    "\n",
    "similarity_relevant"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:14:40.572846Z",
     "end_time": "2024-05-28T15:14:40.817789Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_scores_SOI = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Statement of intent']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    similarity_scores_SOI.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "similarity_SOI = pd.DataFrame(similarity_scores_SOI, columns=['filename', 'Similarity SOI'])\n",
    "similarity_SOI.to_csv(\"STRING_RESULT/B1.0/T0_similarity_scores_SOI\", index=False)\n",
    "\n",
    "similarity_SOI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:14:43.040206Z",
     "end_time": "2024-05-28T15:14:43.250686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarity_scores_NR = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Not relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    similarity_scores_NR.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "similarity_NR = pd.DataFrame(similarity_scores_SOI, columns=['filename', 'Similarity NR'])\n",
    "similarity_NR.to_csv(\"STRING_RESULT/B1.0/T0_similarity_scores_NR\", index=False)\n",
    "\n",
    "similarity_NR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:14:44.398107Z",
     "end_time": "2024-05-28T15:14:45.006347Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation with context specified in text\n",
    "codebook B1.0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B1.0.1', 'r', encoding='utf-8') as file:\n",
    "    B101 = file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T13:49:36.636787Z",
     "end_time": "2024-05-28T13:49:36.666060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B101, key, seed,fingerprint, experiment=\"B1.0.1\",  num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T13:49:39.749149Z",
     "end_time": "2024-05-28T14:05:34.479365Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation B1.0 with context - B1.0.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate through each file in the directory\n",
    "directoryB101 = 'STRING_RESULT/B1.0.1/all_iterations'\n",
    "B101similarity_scores_all = []\n",
    "\n",
    "for filename in os.listdir(directoryB101):\n",
    "    file_path = os.path.join(directoryB101, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    Rx = df['RELEVANCE_x']\n",
    "    Ry = df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx, Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B101similarity_scores_all.append((filename, similarity_score))\n",
    "\n",
    "B101similarity_all = pd.DataFrame(B101similarity_scores_all, columns=['filename', 'similarity ALL'])\n",
    "B101similarity_all.to_csv(\"STRING_RESULT/B1.0.1/T0_similarity_scores_all\", index=False)\n",
    "\n",
    "B101similarity_all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:14:58.617368Z",
     "end_time": "2024-05-28T15:14:58.854503Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B101similarity_scores_relevant = []\n",
    "\n",
    "for filename in os.listdir(directoryB101):\n",
    "    file_path = os.path.join(directoryB101, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B101similarity_scores_relevant.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B101similarity_relevant = pd.DataFrame(B101similarity_scores_relevant, columns=['filename', 'Similarity RELEVANT'])\n",
    "B101similarity_relevant.to_csv(\"STRING_RESULT/B1.0.1/T0_similarity_scores_relevant\", index=False)\n",
    "\n",
    "B101similarity_relevant"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:15:00.784146Z",
     "end_time": "2024-05-28T15:15:00.915499Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B101similarity_scores_relevant_precision = []\n",
    "\n",
    "for filename in os.listdir(directoryB101):\n",
    "    file_path = os.path.join(directoryB101, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_y'] == 'Relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B101similarity_scores_relevant_precision.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B101similarity_relevant_precision = pd.DataFrame(B101similarity_scores_relevant_precision, columns=['filename', 'Similarity RELEVANT'])\n",
    "B101similarity_relevant_precision.to_csv(\"STRING_RESULT/B1.0.1/T0_similarity_scores_relevant_precision\", index=False)\n",
    "\n",
    "B101similarity_relevant_precision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T17:23:31.893204Z",
     "end_time": "2024-05-28T17:23:32.350691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B101similarity_scores_SOI = []\n",
    "\n",
    "for filename in os.listdir(directoryB101):\n",
    "    file_path = os.path.join(directoryB101, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Statement of intent']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B101similarity_scores_SOI.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B101similarity_SOI = pd.DataFrame(B101similarity_scores_SOI, columns=['filename', 'Similarity SOI'])\n",
    "B101similarity_SOI.to_csv(\"STRING_RESULT/B1.0.1/T0_similarity_scores_SOI\", index=False)\n",
    "\n",
    "B101similarity_SOI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:15:03.975529Z",
     "end_time": "2024-05-28T15:15:04.083235Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B101similarity_scores_NR = []\n",
    "\n",
    "for filename in os.listdir(directoryB101):\n",
    "    file_path = os.path.join(directoryB101, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Not relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B101similarity_scores_NR.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B101similarity_NR = pd.DataFrame(B101similarity_scores_SOI, columns=['filename', 'Similarity NR'])\n",
    "B101similarity_NR.to_csv(\"STRING_RESULT/B1.0.1/T0_similarity_scores_NR\", index=False)\n",
    "\n",
    "B101similarity_NR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:09:03.370652Z",
     "end_time": "2024-05-28T15:09:03.605962Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.1 - Oneshot codebook\n",
    "Evaluation with T=0 and 1 iteration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B1.1', 'r', encoding='utf-8') as file:\n",
    "    B11 = file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T14:17:41.064119Z",
     "end_time": "2024-05-28T14:17:41.117450Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B11, key, seed,fingerprint, experiment=\"B1.1\",  num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T14:18:03.418226Z",
     "end_time": "2024-05-28T14:35:04.471909Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation B1.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate through each file in the directory\n",
    "directoryB11 = 'STRING_RESULT/B1.1/all_iterations'\n",
    "B11similarity_scores_all = []\n",
    "\n",
    "for filename in os.listdir(directoryB11):\n",
    "    file_path = os.path.join(directoryB11, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    Rx = df['RELEVANCE_x']\n",
    "    Ry = df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx, Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B11similarity_scores_all.append((filename, similarity_score))\n",
    "\n",
    "B11similarity_all = pd.DataFrame(B11similarity_scores_all, columns=['filename', 'similarity ALL'])\n",
    "B11similarity_all.to_csv(\"STRING_RESULT/B1.1/T0_similarity_scores_all\", index=False)\n",
    "\n",
    "B11similarity_all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:15:20.900819Z",
     "end_time": "2024-05-28T15:15:21.125032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B11similarity_scores_relevant = []\n",
    "\n",
    "for filename in os.listdir(directoryB11):\n",
    "    file_path = os.path.join(directoryB11, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B11similarity_scores_relevant.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B11similarity_relevant = pd.DataFrame(B11similarity_scores_relevant, columns=['filename', 'Similarity RELEVANT'])\n",
    "B11similarity_relevant.to_csv(\"STRING_RESULT/B1.1/T0_similarity_scores_relevant\", index=False)\n",
    "\n",
    "B11similarity_relevant"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:15:24.249144Z",
     "end_time": "2024-05-28T15:15:24.359514Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B11similarity_scores_SOI = []\n",
    "\n",
    "for filename in os.listdir(directoryB11):\n",
    "    file_path = os.path.join(directoryB11, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Statement of intent']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B11similarity_scores_SOI.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B11similarity_SOI = pd.DataFrame(B11similarity_scores_SOI, columns=['filename', 'Similarity SOI'])\n",
    "B11similarity_SOI.to_csv(\"STRING_RESULT/B1.1/T0_similarity_scores_SOI\", index=False)\n",
    "\n",
    "B11similarity_SOI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:15:31.823143Z",
     "end_time": "2024-05-28T15:15:31.937744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B11similarity_scores_NR = []\n",
    "\n",
    "for filename in os.listdir(directoryB11):\n",
    "    file_path = os.path.join(directoryB11, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Not relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B11similarity_scores_NR.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B11similarity_NR = pd.DataFrame(B11similarity_scores_SOI, columns=['filename', 'Similarity NR'])\n",
    "B11similarity_NR.to_csv(\"STRING_RESULT/B1.1/T0_similarity_scores_NR\", index=False)\n",
    "\n",
    "B11similarity_NR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:15:55.053402Z",
     "end_time": "2024-05-28T15:15:55.246512Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.1.1 - Oneshot codebook WITH CONTEXT\n",
    "Evaluation with T=0 and 1 iteration\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load codebook - zero shot\n",
    "with open('codebooks/B1.1.1', 'r', encoding='utf-8') as file:\n",
    "    B111 = file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:16:07.609272Z",
     "end_time": "2024-05-28T15:16:07.639671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Annotate the data - T0 - I1\n",
    "for seed in seeds:\n",
    "    gpt_annotate_string.gpt_annotate(text_to_annotate, B111, key, seed,fingerprint, experiment=\"B1.1.1\",  num_iterations=1, model=\"gpt-4o\", temperature=0, batch_size=20, human_labels=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:16:09.566633Z",
     "end_time": "2024-05-28T15:34:49.180337Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate B111\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate through each file in the directory\n",
    "directoryB111 = 'STRING_RESULT/B1.1.1/all_iterations'\n",
    "B111similarity_scores_all = []\n",
    "\n",
    "for filename in os.listdir(directoryB111):\n",
    "    file_path = os.path.join(directoryB111, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    Rx = df['RELEVANCE_x']\n",
    "    Ry = df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx, Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B111similarity_scores_all.append((filename, similarity_score))\n",
    "\n",
    "B111similarity_all = pd.DataFrame(B111similarity_scores_all, columns=['filename', 'similarity ALL'])\n",
    "B111similarity_all.to_csv(\"STRING_RESULT/B1.1.1/T0_similarity_scores_all\", index=False)\n",
    "\n",
    "B111similarity_all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:36:29.531431Z",
     "end_time": "2024-05-28T15:36:29.995174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B111similarity_scores_relevant = []\n",
    "\n",
    "for filename in os.listdir(directoryB111):\n",
    "    file_path = os.path.join(directoryB111, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B111similarity_scores_relevant.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B111similarity_relevant = pd.DataFrame(B111similarity_scores_relevant, columns=['filename', 'Similarity RELEVANT'])\n",
    "B111similarity_relevant.to_csv(\"STRING_RESULT/B1.1.1/T0_similarity_scores_relevant\", index=False)\n",
    "\n",
    "B111similarity_relevant"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:36:58.559955Z",
     "end_time": "2024-05-28T15:36:58.705570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B111similarity_scores_SOI = []\n",
    "\n",
    "for filename in os.listdir(directoryB111):\n",
    "    file_path = os.path.join(directoryB111, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Statement of intent']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B111similarity_scores_SOI.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B111similarity_SOI = pd.DataFrame(B111similarity_scores_SOI, columns=['filename', 'Similarity SOI'])\n",
    "B111similarity_SOI.to_csv(\"STRING_RESULT/B1.1.1/T0_similarity_scores_SOI\", index=False)\n",
    "\n",
    "B111similarity_SOI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:38:06.639641Z",
     "end_time": "2024-05-28T15:38:06.773981Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B111similarity_scores_NR = []\n",
    "\n",
    "for filename in os.listdir(directoryB111):\n",
    "    file_path = os.path.join(directoryB111, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    relevant_df = df[df['RELEVANCE_x'] == 'Not relevant']\n",
    "\n",
    "    Rx = relevant_df['RELEVANCE_x']\n",
    "    Ry = relevant_df['RELEVANCE_y']\n",
    "\n",
    "    similarity_score = get_similarity_score(Rx,Ry)\n",
    "\n",
    "    #Save the score in a dataframe\n",
    "    B111similarity_scores_NR.append((filename, similarity_score))\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "B111similarity_NR = pd.DataFrame(B111similarity_scores_SOI, columns=['filename', 'Similarity NR'])\n",
    "B111similarity_NR.to_csv(\"STRING_RESULT/B1.1.1/T0_similarity_scores_NR\", index=False)\n",
    "\n",
    "B111similarity_NR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-28T15:38:38.923794Z",
     "end_time": "2024-05-28T15:38:39.178355Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## B1.2 - TWOshot codebook\n",
    "Evaluation with T=0 and 1 iteration\n",
    "\n",
    "Codebook is prepared. Currently not evaluated.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
